{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from scipy.stats import pearsonr, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(conn_mat_filename: str, general_scores_filename: str):\n",
    "    Glasser_conn_mat = np.load(conn_mat_filename)\n",
    "\n",
    "    # get upper triangle indices, without diagonal 0 (k = 1 starts from the k' diagonal)\n",
    "    indices = np.triu_indices(360, k=1)\n",
    "\n",
    "    # create X: rows for subjects, and flattened upper triangle mat for each subject\n",
    "    X = []\n",
    "    for i in range(Glasser_conn_mat.shape[2]):\n",
    "        X.append(Glasser_conn_mat[:, :, i][indices])\n",
    "    # convert X to data frame for pipeline parameters\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    # read scores\n",
    "    y = pd.read_csv(general_scores_filename, header=None).to_numpy()\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y(\"data/Glasser_conn_mat_322_subj.npy\", \"data/general_scores_322_subj.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>64610</th>\n",
       "      <th>64611</th>\n",
       "      <th>64612</th>\n",
       "      <th>64613</th>\n",
       "      <th>64614</th>\n",
       "      <th>64615</th>\n",
       "      <th>64616</th>\n",
       "      <th>64617</th>\n",
       "      <th>64618</th>\n",
       "      <th>64619</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.298940</td>\n",
       "      <td>0.732744</td>\n",
       "      <td>0.918536</td>\n",
       "      <td>0.839563</td>\n",
       "      <td>0.432511</td>\n",
       "      <td>0.442482</td>\n",
       "      <td>0.264298</td>\n",
       "      <td>0.397874</td>\n",
       "      <td>0.363293</td>\n",
       "      <td>0.242166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350374</td>\n",
       "      <td>-0.091814</td>\n",
       "      <td>-0.072425</td>\n",
       "      <td>0.042511</td>\n",
       "      <td>-0.128833</td>\n",
       "      <td>-0.028425</td>\n",
       "      <td>-0.054679</td>\n",
       "      <td>0.021419</td>\n",
       "      <td>-0.038918</td>\n",
       "      <td>0.255055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012735</td>\n",
       "      <td>0.639222</td>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.742386</td>\n",
       "      <td>0.632681</td>\n",
       "      <td>0.491353</td>\n",
       "      <td>0.072223</td>\n",
       "      <td>0.309493</td>\n",
       "      <td>0.335780</td>\n",
       "      <td>0.303365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235892</td>\n",
       "      <td>0.060626</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>0.238003</td>\n",
       "      <td>0.008831</td>\n",
       "      <td>0.105045</td>\n",
       "      <td>0.147294</td>\n",
       "      <td>0.118197</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.386892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.504869</td>\n",
       "      <td>0.793886</td>\n",
       "      <td>0.670200</td>\n",
       "      <td>0.736328</td>\n",
       "      <td>0.275567</td>\n",
       "      <td>0.418920</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>0.645710</td>\n",
       "      <td>0.561706</td>\n",
       "      <td>0.607769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189165</td>\n",
       "      <td>0.301933</td>\n",
       "      <td>0.253142</td>\n",
       "      <td>0.173838</td>\n",
       "      <td>0.151999</td>\n",
       "      <td>0.490085</td>\n",
       "      <td>0.364271</td>\n",
       "      <td>0.171950</td>\n",
       "      <td>0.134088</td>\n",
       "      <td>0.641098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.434921</td>\n",
       "      <td>0.625641</td>\n",
       "      <td>0.866148</td>\n",
       "      <td>0.796666</td>\n",
       "      <td>0.615978</td>\n",
       "      <td>0.449730</td>\n",
       "      <td>0.458218</td>\n",
       "      <td>0.470296</td>\n",
       "      <td>0.479121</td>\n",
       "      <td>0.247320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476037</td>\n",
       "      <td>0.116935</td>\n",
       "      <td>0.211443</td>\n",
       "      <td>0.206184</td>\n",
       "      <td>0.161278</td>\n",
       "      <td>0.283144</td>\n",
       "      <td>0.201882</td>\n",
       "      <td>0.168905</td>\n",
       "      <td>0.232736</td>\n",
       "      <td>0.466078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.329170</td>\n",
       "      <td>0.684801</td>\n",
       "      <td>0.879460</td>\n",
       "      <td>0.848767</td>\n",
       "      <td>0.750146</td>\n",
       "      <td>0.717157</td>\n",
       "      <td>0.090091</td>\n",
       "      <td>0.039740</td>\n",
       "      <td>0.348103</td>\n",
       "      <td>0.159279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473147</td>\n",
       "      <td>0.021511</td>\n",
       "      <td>0.135669</td>\n",
       "      <td>0.235904</td>\n",
       "      <td>0.133697</td>\n",
       "      <td>0.182638</td>\n",
       "      <td>0.183275</td>\n",
       "      <td>0.171366</td>\n",
       "      <td>0.136573</td>\n",
       "      <td>0.284417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.614404</td>\n",
       "      <td>0.776613</td>\n",
       "      <td>0.931250</td>\n",
       "      <td>0.912197</td>\n",
       "      <td>0.827961</td>\n",
       "      <td>0.740271</td>\n",
       "      <td>0.632516</td>\n",
       "      <td>0.538272</td>\n",
       "      <td>0.654627</td>\n",
       "      <td>0.579273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221746</td>\n",
       "      <td>0.028976</td>\n",
       "      <td>0.111459</td>\n",
       "      <td>0.018839</td>\n",
       "      <td>0.042003</td>\n",
       "      <td>-0.053316</td>\n",
       "      <td>-0.165810</td>\n",
       "      <td>0.195446</td>\n",
       "      <td>0.243251</td>\n",
       "      <td>0.484157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.206464</td>\n",
       "      <td>0.591203</td>\n",
       "      <td>0.831999</td>\n",
       "      <td>0.796237</td>\n",
       "      <td>0.584414</td>\n",
       "      <td>0.506734</td>\n",
       "      <td>0.296378</td>\n",
       "      <td>0.323051</td>\n",
       "      <td>0.333396</td>\n",
       "      <td>0.076793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570339</td>\n",
       "      <td>0.149866</td>\n",
       "      <td>0.088419</td>\n",
       "      <td>0.147588</td>\n",
       "      <td>0.335105</td>\n",
       "      <td>0.178705</td>\n",
       "      <td>0.190511</td>\n",
       "      <td>0.100918</td>\n",
       "      <td>0.140988</td>\n",
       "      <td>0.321525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.144381</td>\n",
       "      <td>0.508423</td>\n",
       "      <td>0.848602</td>\n",
       "      <td>0.766589</td>\n",
       "      <td>0.585104</td>\n",
       "      <td>0.261164</td>\n",
       "      <td>0.107827</td>\n",
       "      <td>0.285483</td>\n",
       "      <td>0.113639</td>\n",
       "      <td>0.034838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573053</td>\n",
       "      <td>0.367515</td>\n",
       "      <td>0.210395</td>\n",
       "      <td>0.039314</td>\n",
       "      <td>0.256437</td>\n",
       "      <td>0.520648</td>\n",
       "      <td>0.165135</td>\n",
       "      <td>0.057304</td>\n",
       "      <td>0.048039</td>\n",
       "      <td>0.474967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.237676</td>\n",
       "      <td>0.520094</td>\n",
       "      <td>0.939677</td>\n",
       "      <td>0.912065</td>\n",
       "      <td>0.806352</td>\n",
       "      <td>0.690207</td>\n",
       "      <td>0.069966</td>\n",
       "      <td>-0.150622</td>\n",
       "      <td>0.494487</td>\n",
       "      <td>0.408926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354727</td>\n",
       "      <td>0.017433</td>\n",
       "      <td>-0.069727</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>-0.132383</td>\n",
       "      <td>0.055792</td>\n",
       "      <td>0.062163</td>\n",
       "      <td>0.197313</td>\n",
       "      <td>0.133259</td>\n",
       "      <td>0.697519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.508142</td>\n",
       "      <td>0.654657</td>\n",
       "      <td>0.917090</td>\n",
       "      <td>0.871074</td>\n",
       "      <td>0.814804</td>\n",
       "      <td>0.707210</td>\n",
       "      <td>0.246177</td>\n",
       "      <td>0.038907</td>\n",
       "      <td>0.492957</td>\n",
       "      <td>0.437851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286061</td>\n",
       "      <td>0.032360</td>\n",
       "      <td>0.071156</td>\n",
       "      <td>0.180542</td>\n",
       "      <td>0.178882</td>\n",
       "      <td>0.144709</td>\n",
       "      <td>0.126925</td>\n",
       "      <td>0.152877</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.365016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 64620 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6      \\\n",
       "0    0.298940  0.732744  0.918536  0.839563  0.432511  0.442482  0.264298   \n",
       "1    0.012735  0.639222  0.825878  0.742386  0.632681  0.491353  0.072223   \n",
       "2    0.504869  0.793886  0.670200  0.736328  0.275567  0.418920  0.662700   \n",
       "3    0.434921  0.625641  0.866148  0.796666  0.615978  0.449730  0.458218   \n",
       "4    0.329170  0.684801  0.879460  0.848767  0.750146  0.717157  0.090091   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "317  0.614404  0.776613  0.931250  0.912197  0.827961  0.740271  0.632516   \n",
       "318  0.206464  0.591203  0.831999  0.796237  0.584414  0.506734  0.296378   \n",
       "319  0.144381  0.508423  0.848602  0.766589  0.585104  0.261164  0.107827   \n",
       "320  0.237676  0.520094  0.939677  0.912065  0.806352  0.690207  0.069966   \n",
       "321  0.508142  0.654657  0.917090  0.871074  0.814804  0.707210  0.246177   \n",
       "\n",
       "        7         8         9      ...     64610     64611     64612  \\\n",
       "0    0.397874  0.363293  0.242166  ...  0.350374 -0.091814 -0.072425   \n",
       "1    0.309493  0.335780  0.303365  ...  0.235892  0.060626  0.244300   \n",
       "2    0.645710  0.561706  0.607769  ...  0.189165  0.301933  0.253142   \n",
       "3    0.470296  0.479121  0.247320  ...  0.476037  0.116935  0.211443   \n",
       "4    0.039740  0.348103  0.159279  ...  0.473147  0.021511  0.135669   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "317  0.538272  0.654627  0.579273  ...  0.221746  0.028976  0.111459   \n",
       "318  0.323051  0.333396  0.076793  ...  0.570339  0.149866  0.088419   \n",
       "319  0.285483  0.113639  0.034838  ...  0.573053  0.367515  0.210395   \n",
       "320 -0.150622  0.494487  0.408926  ...  0.354727  0.017433 -0.069727   \n",
       "321  0.038907  0.492957  0.437851  ...  0.286061  0.032360  0.071156   \n",
       "\n",
       "        64613     64614     64615     64616     64617     64618     64619  \n",
       "0    0.042511 -0.128833 -0.028425 -0.054679  0.021419 -0.038918  0.255055  \n",
       "1    0.238003  0.008831  0.105045  0.147294  0.118197  0.103300  0.386892  \n",
       "2    0.173838  0.151999  0.490085  0.364271  0.171950  0.134088  0.641098  \n",
       "3    0.206184  0.161278  0.283144  0.201882  0.168905  0.232736  0.466078  \n",
       "4    0.235904  0.133697  0.182638  0.183275  0.171366  0.136573  0.284417  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "317  0.018839  0.042003 -0.053316 -0.165810  0.195446  0.243251  0.484157  \n",
       "318  0.147588  0.335105  0.178705  0.190511  0.100918  0.140988  0.321525  \n",
       "319  0.039314  0.256437  0.520648  0.165135  0.057304  0.048039  0.474967  \n",
       "320 -0.020538 -0.132383  0.055792  0.062163  0.197313  0.133259  0.697519  \n",
       "321  0.180542  0.178882  0.144709  0.126925  0.152877  0.151000  0.365016  \n",
       "\n",
       "[322 rows x 64620 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[613.],\n",
       "       [724.],\n",
       "       [703.],\n",
       "       [668.],\n",
       "       [690.],\n",
       "       [698.],\n",
       "       [711.],\n",
       "       [759.],\n",
       "       [513.],\n",
       "       [728.],\n",
       "       [630.],\n",
       "       [596.],\n",
       "       [678.],\n",
       "       [690.],\n",
       "       [610.],\n",
       "       [623.],\n",
       "       [530.],\n",
       "       [713.],\n",
       "       [517.],\n",
       "       [635.],\n",
       "       [627.],\n",
       "       [630.],\n",
       "       [615.],\n",
       "       [664.],\n",
       "       [620.],\n",
       "       [776.],\n",
       "       [741.],\n",
       "       [692.],\n",
       "       [663.],\n",
       "       [620.],\n",
       "       [685.],\n",
       "       [698.],\n",
       "       [770.],\n",
       "       [687.],\n",
       "       [522.],\n",
       "       [620.],\n",
       "       [690.],\n",
       "       [729.],\n",
       "       [690.],\n",
       "       [732.],\n",
       "       [604.],\n",
       "       [721.],\n",
       "       [721.],\n",
       "       [721.],\n",
       "       [550.],\n",
       "       [750.],\n",
       "       [600.],\n",
       "       [730.],\n",
       "       [667.],\n",
       "       [595.],\n",
       "       [697.],\n",
       "       [672.],\n",
       "       [699.],\n",
       "       [718.],\n",
       "       [728.],\n",
       "       [581.],\n",
       "       [658.],\n",
       "       [689.],\n",
       "       [650.],\n",
       "       [641.],\n",
       "       [660.],\n",
       "       [729.],\n",
       "       [678.],\n",
       "       [625.],\n",
       "       [726.],\n",
       "       [742.],\n",
       "       [652.],\n",
       "       [640.],\n",
       "       [620.],\n",
       "       [640.],\n",
       "       [500.],\n",
       "       [750.],\n",
       "       [673.],\n",
       "       [675.],\n",
       "       [695.],\n",
       "       [677.],\n",
       "       [630.],\n",
       "       [696.],\n",
       "       [741.],\n",
       "       [651.],\n",
       "       [695.],\n",
       "       [577.],\n",
       "       [600.],\n",
       "       [690.],\n",
       "       [582.],\n",
       "       [564.],\n",
       "       [664.],\n",
       "       [712.],\n",
       "       [490.],\n",
       "       [500.],\n",
       "       [729.],\n",
       "       [590.],\n",
       "       [702.],\n",
       "       [734.],\n",
       "       [745.],\n",
       "       [647.],\n",
       "       [709.],\n",
       "       [648.],\n",
       "       [670.],\n",
       "       [747.],\n",
       "       [671.],\n",
       "       [692.],\n",
       "       [596.],\n",
       "       [738.],\n",
       "       [765.],\n",
       "       [650.],\n",
       "       [786.],\n",
       "       [706.],\n",
       "       [689.],\n",
       "       [676.],\n",
       "       [670.],\n",
       "       [623.],\n",
       "       [622.],\n",
       "       [780.],\n",
       "       [612.],\n",
       "       [698.],\n",
       "       [726.],\n",
       "       [663.],\n",
       "       [665.],\n",
       "       [719.],\n",
       "       [734.],\n",
       "       [728.],\n",
       "       [667.],\n",
       "       [723.],\n",
       "       [631.],\n",
       "       [768.],\n",
       "       [623.],\n",
       "       [750.],\n",
       "       [729.],\n",
       "       [740.],\n",
       "       [532.],\n",
       "       [630.],\n",
       "       [738.],\n",
       "       [734.],\n",
       "       [650.],\n",
       "       [729.],\n",
       "       [745.],\n",
       "       [585.],\n",
       "       [692.],\n",
       "       [669.],\n",
       "       [737.],\n",
       "       [658.],\n",
       "       [711.],\n",
       "       [730.],\n",
       "       [670.],\n",
       "       [728.],\n",
       "       [656.],\n",
       "       [641.],\n",
       "       [657.],\n",
       "       [698.],\n",
       "       [763.],\n",
       "       [697.],\n",
       "       [723.],\n",
       "       [650.],\n",
       "       [680.],\n",
       "       [600.],\n",
       "       [685.],\n",
       "       [718.],\n",
       "       [678.],\n",
       "       [584.],\n",
       "       [640.],\n",
       "       [730.],\n",
       "       [630.],\n",
       "       [633.],\n",
       "       [680.],\n",
       "       [632.],\n",
       "       [640.],\n",
       "       [699.],\n",
       "       [540.],\n",
       "       [670.],\n",
       "       [724.],\n",
       "       [680.],\n",
       "       [744.],\n",
       "       [600.],\n",
       "       [670.],\n",
       "       [703.],\n",
       "       [701.],\n",
       "       [734.],\n",
       "       [752.],\n",
       "       [693.],\n",
       "       [683.],\n",
       "       [709.],\n",
       "       [728.],\n",
       "       [763.],\n",
       "       [710.],\n",
       "       [678.],\n",
       "       [690.],\n",
       "       [677.],\n",
       "       [719.],\n",
       "       [669.],\n",
       "       [730.],\n",
       "       [732.],\n",
       "       [710.],\n",
       "       [695.],\n",
       "       [756.],\n",
       "       [703.],\n",
       "       [646.],\n",
       "       [700.],\n",
       "       [727.],\n",
       "       [681.],\n",
       "       [687.],\n",
       "       [685.],\n",
       "       [759.],\n",
       "       [625.],\n",
       "       [732.],\n",
       "       [720.],\n",
       "       [624.],\n",
       "       [661.],\n",
       "       [776.],\n",
       "       [747.],\n",
       "       [756.],\n",
       "       [725.],\n",
       "       [640.],\n",
       "       [719.],\n",
       "       [680.],\n",
       "       [680.],\n",
       "       [734.],\n",
       "       [723.],\n",
       "       [708.],\n",
       "       [726.],\n",
       "       [714.],\n",
       "       [711.],\n",
       "       [657.],\n",
       "       [649.],\n",
       "       [736.],\n",
       "       [470.],\n",
       "       [629.],\n",
       "       [700.],\n",
       "       [690.],\n",
       "       [576.],\n",
       "       [682.],\n",
       "       [540.],\n",
       "       [642.],\n",
       "       [603.],\n",
       "       [655.],\n",
       "       [605.],\n",
       "       [622.],\n",
       "       [670.],\n",
       "       [660.],\n",
       "       [634.],\n",
       "       [543.],\n",
       "       [666.],\n",
       "       [591.],\n",
       "       [792.],\n",
       "       [588.],\n",
       "       [520.],\n",
       "       [650.],\n",
       "       [647.],\n",
       "       [643.],\n",
       "       [538.],\n",
       "       [650.],\n",
       "       [631.],\n",
       "       [660.],\n",
       "       [660.],\n",
       "       [584.],\n",
       "       [618.],\n",
       "       [668.],\n",
       "       [580.],\n",
       "       [586.],\n",
       "       [650.],\n",
       "       [645.],\n",
       "       [620.],\n",
       "       [640.],\n",
       "       [575.],\n",
       "       [618.],\n",
       "       [625.],\n",
       "       [570.],\n",
       "       [620.],\n",
       "       [600.],\n",
       "       [658.],\n",
       "       [634.],\n",
       "       [590.],\n",
       "       [565.],\n",
       "       [651.],\n",
       "       [656.],\n",
       "       [447.],\n",
       "       [587.],\n",
       "       [649.],\n",
       "       [633.],\n",
       "       [648.],\n",
       "       [610.],\n",
       "       [635.],\n",
       "       [610.],\n",
       "       [560.],\n",
       "       [657.],\n",
       "       [598.],\n",
       "       [650.],\n",
       "       [523.],\n",
       "       [518.],\n",
       "       [581.],\n",
       "       [635.],\n",
       "       [573.],\n",
       "       [625.],\n",
       "       [647.],\n",
       "       [626.],\n",
       "       [605.],\n",
       "       [604.],\n",
       "       [614.],\n",
       "       [664.],\n",
       "       [582.],\n",
       "       [630.],\n",
       "       [516.],\n",
       "       [540.],\n",
       "       [630.],\n",
       "       [575.],\n",
       "       [620.],\n",
       "       [617.],\n",
       "       [647.],\n",
       "       [621.],\n",
       "       [547.],\n",
       "       [535.],\n",
       "       [503.],\n",
       "       [672.],\n",
       "       [630.],\n",
       "       [621.],\n",
       "       [528.],\n",
       "       [540.],\n",
       "       [650.],\n",
       "       [640.],\n",
       "       [663.],\n",
       "       [540.],\n",
       "       [640.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean General Score: 658.28\n",
      "Standard deviation: 64.65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean General Score: {np.round(np.mean(y), 2)}\")\n",
    "print(f\"Standard deviation: {np.round(np.std(y), 2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation negative MSE score:  [-1.13 -1.22 -0.77 -1.5  -0.97 -1.18 -0.9  -0.91 -1.53 -1.47]\n",
      "mean MSE across folds: 1.16\n",
      "MSE standard error across folds: 0.09\n",
      "Test MSE: 0.589, r: 0.309, p value: 0.08\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Report cross validation score across all folds\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation negative MSE score: \", np.round(cross_val_score_list, 2))\n",
    "print(\"mean MSE across folds:\", np.round(-1 * np.mean(cross_val_score_list), 2))\n",
    "print(\"MSE standard error across folds:\", np.round(sem(cross_val_score_list), 2))\n",
    "\n",
    "# Fit the model to train data \n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on unseen test set\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation negative MSE score:  [-1.13 -1.22 -0.77 -1.5  -0.97 -1.18 -0.9  -0.91 -1.53 -1.47]\n",
      "mean MSE across folds: 1.16\n",
      "MSE standard error across folds: 0.09\n",
      "Test MSE: 0.589, r: 0.309, p value: 0.08\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "rdg = Ridge()\n",
    "\n",
    "# Report cross validation score across all folds\n",
    "cross_val_score_list = cross_val_score(rdg, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation negative MSE score: \", np.round(cross_val_score_list, 2))\n",
    "print(\"mean MSE across folds:\", np.round(-1 * np.mean(cross_val_score_list), 2))\n",
    "print(\"MSE standard error across folds:\", np.round(sem(cross_val_score_list), 2))\n",
    "\n",
    "# Fit the model to train data \n",
    "rdg.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on unseen test set\n",
    "y_test_predicted = rdg.predict(X_test_pca)\n",
    "test_mse_rdg = mean_squared_error(y_test, y_test_predicted)\n",
    "r_rdg, p_val_rdg = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_rdg, 3)}, r: {np.round(r_rdg, 3)}, p value: {np.round(p_val_rdg, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation negative MSE score:  [-0.98 -1.2  -0.75 -1.39 -0.9  -0.99 -0.87 -0.9  -1.11 -1.43]\n",
      "mean MSE across folds: 1.05\n",
      "MSE standard error across folds: 0.07\n",
      "Test MSE: 0.626, r: 0.224, p value: 0.21\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = Lasso()\n",
    "\n",
    "# Report cross validation score across all folds\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation negative MSE score: \", np.round(cross_val_score_list, 2))\n",
    "print(\"mean MSE across folds:\", np.round(-1 * np.mean(cross_val_score_list), 2))\n",
    "print(\"MSE standard error across folds:\", np.round(sem(cross_val_score_list), 2))\n",
    "\n",
    "# Fit the model to train data\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on unseen test set\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation negative MSE score:  [-1.04 -1.21 -0.76 -1.44 -0.93 -1.07 -0.87 -0.9  -1.23 -1.43]\n",
      "mean MSE across folds: 1.09\n",
      "MSE standard error across folds: 0.07\n",
      "Test MSE: 0.604, r: 0.287, p value: 0.105\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = ElasticNet()\n",
    "\n",
    "# Report cross validation score across all folds\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation negative MSE score: \", np.round(cross_val_score_list, 2))\n",
    "print(\"mean MSE across folds:\", np.round(-1 * np.mean(cross_val_score_list), 2))\n",
    "print(\"MSE standard error across folds:\", np.round(sem(cross_val_score_list), 2))\n",
    "\n",
    "# Fit the model to train data\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on unseen test set\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r:  0.15 , mean p-value:  0.39\n",
      "min MSE: 0.69 in fold 6\n",
      "{'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "cross validation negative MSE score:  [-0.79 -1.11 -0.71 -1.27 -0.79 -0.82 -0.87 -0.98 -0.88 -1.29]\n",
      "mean MSE across folds: 0.95\n",
      "MSE standard error across folds: 0.07\n",
      "MSE: 0.551, r: 0.298, p value: 0.092\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=75, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Parameters grid for hyperparameter tuning \n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "r_vec = []\n",
    "p_value_vec = []\n",
    "fold_mse_train = []\n",
    "fold_mse_val = []\n",
    "fold_config = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_pca):\n",
    "    X_train_fold, X_val_fold = X_train_pca[train_index], X_train_pca[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    random_forest = RandomForestRegressor(random_state=0)\n",
    "    grid_search = GridSearchCV(\n",
    "        random_forest,\n",
    "        param_grid=param_grid_rf,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Fit and Predict\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_train_predicted = best_estimator.predict(X_train_fold)\n",
    "    y_predicted = best_estimator.predict(X_val_fold)\n",
    "    fold_config.append(grid_search.best_params_)\n",
    "    train_mse, val_mse = mean_squared_error(y_train_fold, y_train_predicted), mean_squared_error(y_val_fold, y_predicted)\n",
    "\n",
    "    r, p_value = pearsonr(y_predicted, y_val_fold)\n",
    "    r_vec.append(r)\n",
    "    p_value_vec.append(p_value)\n",
    "    fold_mse_train.append(train_mse)\n",
    "    fold_mse_val.append(val_mse)\n",
    "\n",
    "print(\"mean r: \", np.round(np.mean(r_vec), 2), \", mean p-value: \", np.round(np.mean(p_value_vec), 2))\n",
    "\n",
    "print(f\"min MSE: {np.round(np.min(fold_mse_val), 2)} in fold {np.argmin(fold_mse_val) + 1}\")\n",
    "# Configuration that received the minimal MSE\n",
    "config = fold_config[np.argmin(fold_mse_val)]\n",
    "print(config)\n",
    "\n",
    "# Random Forest Regressor with the most accurate configuration\n",
    "chosen_random_forest = RandomForestRegressor(random_state=0, **config)\n",
    "\n",
    "# Report cross validation score across all folds\n",
    "cross_val_score_list = cross_val_score(chosen_random_forest, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation negative MSE score: \", np.round(cross_val_score_list, 2))\n",
    "print(\"mean MSE across folds:\", np.round(-1 * np.mean(cross_val_score_list), 2))\n",
    "print(\"MSE standard error across folds:\", np.round(sem(cross_val_score_list), 2))\n",
    "\n",
    "# Fit the model to train data\n",
    "chosen_random_forest.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on unseen test set\n",
    "y_pred = chosen_random_forest.predict(X_test_pca)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r, p_value = pearsonr(y_test, y_pred)\n",
    "print(f\"MSE: {np.round(mse, 3)}, r: {np.round(r, 3)}, p value: {np.round(p_value, 3)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation negative MSE score:  [-0.89 -1.41 -0.84 -1.69 -1.19 -1.48 -1.17 -0.88 -1.2  -1.45]\n",
      "mean MSE across folds: 1.22\n",
      "MSE standard error across folds: 0.09\n",
      "Test MSE: 0.503, r: 0.37, p value: 0.034\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "# Report cross validation score across all folds\n",
    "cross_val_score_list = cross_val_score(gbr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation negative MSE score: \", np.round(cross_val_score_list, 2))\n",
    "print(\"mean MSE across folds:\", np.round(-1 * np.mean(cross_val_score_list), 2))\n",
    "print(\"MSE standard error across folds:\", np.round(sem(cross_val_score_list), 2))\n",
    "\n",
    "# Fit the model to train data\n",
    "gbr.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on unseen test set\n",
    "y_test_predicted = gbr.predict(X_test_pca)\n",
    "test_mse_gbr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_gbr, p_val_gbr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_gbr, 3)}, r: {np.round(r_gbr, 3)}, p value: {np.round(p_val_gbr, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
